{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awe.utils\n",
    "awe.utils.init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awe.data.parsing\n",
    "import awe.data.set.apify\n",
    "import awe.data.set.pages\n",
    "import awe.data.set.swde\n",
    "import awe.data.validation\n",
    "import awe.data.visual.exploration\n",
    "awe.utils.reload('awe.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Choose between the first two or the last two cells in this section depending on whether you want to load the Apify or the SWDE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = awe.data.set.apify.Dataset(\n",
    "    # only_label_keys=('name', 'price', 'category', 'images'),\n",
    "    # convert=False,\n",
    "    # convert_slim=True,\n",
    "    # skip_without_visuals=True,\n",
    "    # only_websites=('notinoEn',),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p.row for p in ds.get_all_pages()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = awe.data.set.swde.Dataset(\n",
    "    suffix='-exact',\n",
    "    only_verticals=('auto',),\n",
    "    # convert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p.to_row() for p in ds.get_all_pages()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label statistics\n",
    "\n",
    "This section shows a table with number of labeled nodes in each website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_stats(page: awe.data.set.pages.Page):\n",
    "    page_labels = page.get_labels()\n",
    "    return sum(\n",
    "        (\n",
    "            collections.Counter({\n",
    "                k: len(page_labels.get_label_values(k)),\n",
    "                #f'{k}_nodes': len(page_labels.get_labeled_nodes(k)),\n",
    "            })\n",
    "            for k in page_labels.label_keys\n",
    "        ),\n",
    "        collections.Counter()\n",
    "    )\n",
    "\n",
    "stats = [\n",
    "    sum(\n",
    "        (\n",
    "            get_label_stats(p)\n",
    "            for p in tqdm(w.pages, desc=w.name, disable=True)\n",
    "            if w.page_count != 0\n",
    "        ),\n",
    "        collections.Counter()\n",
    "    )\n",
    "    for w in tqdm(ds.verticals[0].websites, desc='websites')\n",
    "]\n",
    "keys = { k for s in stats for k in s.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'website': w.name,\n",
    "        'domain': w.get_domain(),\n",
    "        'pages': w.page_count,\n",
    "    }\n",
    "    | {\n",
    "        k: 0\n",
    "        for k in keys\n",
    "    } | {\n",
    "        k: c #f'{c} ({c / w.page_count:.0%})'\n",
    "        for k, c in s.items()\n",
    "    }\n",
    "    for w, s in zip(ds.verticals[0].websites, stats)\n",
    "])\n",
    "# Add totals.\n",
    "df.loc['total'] = df.sum()\n",
    "df.loc['total', ('website', 'domain')] = ''\n",
    "# Sort columns by name.\n",
    "df = df.reindex(\n",
    "    sorted(\n",
    "        df.columns,\n",
    "        key=lambda n: f'_{n}' if n in ('website', 'domain', 'pages') else n\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the table is turned into LaTeX code to be used in the thesis (to avoid manual and error-prone filling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table.\n",
    "if ds.name == 'apify':\n",
    "    columns = ('name', 'price', 'category', 'images', 'shortDescription', 'longDescription', 'specification')\n",
    "    display_cols = {\n",
    "        'category': 'cat',\n",
    "        'shortDescription': 'short',\n",
    "        'longDescription': 'long',\n",
    "        'specification': 'spec'\n",
    "    }\n",
    "    split_nv = False\n",
    "else:\n",
    "    columns = ('model', 'price', 'engine', 'fuel_economy')\n",
    "    display_cols = {\n",
    "        'fuel_economy': 'economy'\n",
    "    }\n",
    "    split_nv = True\n",
    "if split_nv:\n",
    "    print('&', end=' ')\n",
    "else:\n",
    "    print('website & pages', end=' ')\n",
    "for col in columns:\n",
    "    col = display_cols.get(col, col)\n",
    "    print(f'& \\\\akcol{{{col}}}', end=' ')\n",
    "print('\\\\\\\\')\n",
    "if split_nv:\n",
    "    print('website & pages', end=' ')\n",
    "    for col in columns:\n",
    "        print('& \\\\akvn', end=' ')\n",
    "    print('\\\\\\\\')\n",
    "print('\\\\midrule')\n",
    "for w, s in zip(ds.verticals[0].websites, stats):\n",
    "    name = w.get_domain() \\\n",
    "        .removeprefix('www.') \\\n",
    "        .removesuffix('.com') \\\n",
    "        .removesuffix('.co.uk')\n",
    "    print(f'\\\\verb|{name}|', end=' ')\n",
    "    print(f'& {w.page_count:,}', end=' ')\n",
    "    for col in columns:\n",
    "        if split_nv:\n",
    "            print(f'& {s[col]:,}', end=' ')\n",
    "        print(f'& {s[f\"{col}_nodes\"]:,}', end=' ')\n",
    "    print('\\\\\\\\')\n",
    "print('\\\\bottomrule')\n",
    "print('total', end=' ')\n",
    "print(f'& {sum(w.page_count for w in ds.verticals[0].websites):,}', end=' ')\n",
    "for col in columns:\n",
    "    if split_nv:\n",
    "        print(f'& {sum(s[col] for s in stats):,}', end=' ')\n",
    "    print(f'& {sum(s[f\"{col}_nodes\"] for s in stats):,}', end=' ')\n",
    "print('\\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node statistics\n",
    "\n",
    "This section shows table with median number of DOM nodes in each website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes (median across pages) in each website.\n",
    "def get_num_nodes(page: awe.data.set.pages.Page):\n",
    "    html_text = page.get_html_text()\n",
    "    tree = awe.data.parsing.parse_html(html_text)\n",
    "    awe.data.parsing.filter_tree(tree)\n",
    "    nodes = tree.root.traverse(include_text=True)\n",
    "    return sum(1 for _ in nodes)\n",
    "def get_median_nodes(website: awe.data.set.pages.Website):\n",
    "    return math.floor(statistics.median(\n",
    "        get_num_nodes(p)\n",
    "        for p in website.pages\n",
    "    ))\n",
    "median_stats = [\n",
    "    get_median_nodes(w)\n",
    "    for w in tqdm(ds.verticals[0].websites, desc='websites')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'website': w.name,\n",
    "        'domain': w.get_domain(),\n",
    "        'pages': w.page_count,\n",
    "        'nodes': m,\n",
    "    }\n",
    "    for w, m in zip(ds.verticals[0].websites, median_stats)\n",
    "])\n",
    "print(f'Average: {df.nodes.mean()}')\n",
    "print(f'std: {df.nodes.std()}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshots\n",
    "\n",
    "This section shows screenshots of pages with target nodes highlighted (by drawing their bounding boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = ds.verticals[0].websites\n",
    "_ = awe.data.visual.exploration.plot_websites(websites, n_cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML tag statistics\n",
    "\n",
    "This section shows the distribution of HTML tag names labeled as `images` (in the Apify dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which HTML tags are labeled as images?\n",
    "def get_page_dom(page: awe.data.set.pages.Page):\n",
    "    page_dom = page.dom\n",
    "    if page_dom.root is None:\n",
    "        page_dom.init_nodes()\n",
    "        page_dom.init_labels(propagate_to_leaves=True)\n",
    "    return page_dom\n",
    "rng = np.random.default_rng(42)\n",
    "{\n",
    "    w.name: collections.Counter(\n",
    "        html_tag\n",
    "        for p in rng.choice(w.pages, 5, replace=False)\n",
    "        for html_tag in set(\n",
    "            node.html_tag\n",
    "            for labeled_nodes in get_page_dom(p).labeled_nodes.get('images', ())\n",
    "            for node in labeled_nodes\n",
    "        )\n",
    "    )\n",
    "    for w in tqdm(ds.verticals[0].websites, desc='websites')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOM exploration\n",
    "\n",
    "This section begins exploring DOM of one page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a page\n",
    "\n",
    "A page can be either loaded from the external list of invalid pages (produced by our validation code)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/invalid_pages.txt', mode='r', encoding='utf-8') as f:\n",
    "    file_path = f.readline().rstrip()\n",
    "page = next(p for p in ds.get_all_pages() if p.original_html_path == file_path)\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or simply one sample selected from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = ds.verticals[0].websites[0].pages[0]\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare page\n",
    "\n",
    "Here, page DOM and visuals are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.url, page.html_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.clear_cache(awe.data.set.pages.ClearCacheRequest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_labels = page.get_labels()\n",
    "page_dom = page.cache_dom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.init_nodes()\n",
    "len(page_dom.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_visuals = page.load_visuals()\n",
    "page_visuals.fill_tree(page_dom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.filter_nodes()\n",
    "len(page_dom.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all text fragments with visuals as \"sampled\".\n",
    "for node in page_dom.nodes:\n",
    "    node.sample = node.is_text and node.box is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore labels\n",
    "\n",
    "This section shows labeled nodes in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ k: v for k, v in page.row.items() if k.startswith('selector_') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.init_labels(propagate_to_leaves=True)\n",
    "{\n",
    "    k: [[n.get_xpath() for n in g] for g in v]\n",
    "    for k, v in page_dom.labeled_nodes.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "        'label_key': k,\n",
    "        'xpath': n.get_xpath(),\n",
    "        'text': n.parsed.text(),\n",
    "        'tag': n.find_semantic_html_tag(),\n",
    "        'box': n.box.as_tuple()\n",
    "    }\n",
    "    for k, v in page.dom.labeled_nodes.items()\n",
    "    for g in v\n",
    "    for n in g[:1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k: page_labels.get_label_values(k)\n",
    "    for k in page_labels.label_keys\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k: [\n",
    "        n.text()\n",
    "        for n in page_labels.get_labeled_nodes(k)\n",
    "    ]\n",
    "    for k in page_labels.label_keys\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual neighbors\n",
    "\n",
    "This section shows visual neighbors of target nodes (what the model will see)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.compute_visual_neighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.compute_visual_neighbors_rect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "        'label_key': k,\n",
    "        'text': n.parsed.text()\n",
    "    } | {\n",
    "        f'neighbor_{i}': (m.distance_x, m.distance_y, m.neighbor.get_text_or_tag())\n",
    "        for i, m in enumerate(n.visual_neighbors)\n",
    "    }\n",
    "    for k, v in page.dom.labeled_nodes.items()\n",
    "    for g in v\n",
    "    for n in g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friend cycles\n",
    "\n",
    "This section shows friends of target nodes (what the model will see)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dom.compute_friend_cycles(max_ancestor_distance=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_node = page_dom.labeled_nodes['price'][0][0]\n",
    "text_nodes = [n for n in price_node.traverse() if n.is_text]\n",
    "[(n.text, n.partner.text if n.partner else None) for n in text_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = text_nodes[0]\n",
    "pd.DataFrame([{\n",
    "    'tag': n.html_tag,\n",
    "    'index': n.deep_index,\n",
    "    'distance': n.deep_index - target_node.deep_index,\n",
    "    'text': n.parsed.text()\n",
    " } for n in target_node.friends or ()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
